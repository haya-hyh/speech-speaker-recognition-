{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4176e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77454a06",
   "metadata": {},
   "source": [
    "# dynamic lmfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c50ff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.8192, Val Loss = 0.7196, Val Acc = 0.7609\n",
      "Epoch 2: Train Loss = 0.6495, Val Loss = 0.6802, Val Acc = 0.7701\n",
      "Epoch 3: Train Loss = 0.6011, Val Loss = 0.6476, Val Acc = 0.7846\n",
      "Epoch 4: Train Loss = 0.5678, Val Loss = 0.6452, Val Acc = 0.7853\n",
      "Epoch 5: Train Loss = 0.5490, Val Loss = 0.6478, Val Acc = 0.7860\n",
      "Epoch 6: Train Loss = 0.5354, Val Loss = 0.6426, Val Acc = 0.7875\n",
      "Epoch 7: Train Loss = 0.5256, Val Loss = 0.6360, Val Acc = 0.7911\n",
      "Epoch 8: Train Loss = 0.5174, Val Loss = 0.6430, Val Acc = 0.7891\n",
      "Epoch 9: Train Loss = 0.5104, Val Loss = 0.6505, Val Acc = 0.7886\n",
      "Epoch 10: Train Loss = 0.5043, Val Loss = 0.6467, Val Acc = 0.7921\n",
      "Epoch 11: Train Loss = 0.4994, Val Loss = 0.6474, Val Acc = 0.7907\n",
      "Epoch 12: Train Loss = 0.4946, Val Loss = 0.6369, Val Acc = 0.7942\n",
      "Epoch 13: Train Loss = 0.4907, Val Loss = 0.6445, Val Acc = 0.7929\n",
      "Epoch 14: Train Loss = 0.4874, Val Loss = 0.6682, Val Acc = 0.7884\n",
      "Epoch 15: Train Loss = 0.4841, Val Loss = 0.6556, Val Acc = 0.7920\n",
      "Epoch 16: Train Loss = 0.4808, Val Loss = 0.6550, Val Acc = 0.7925\n",
      "Epoch 17: Train Loss = 0.4786, Val Loss = 0.6579, Val Acc = 0.7920\n",
      "Epoch 18: Train Loss = 0.4759, Val Loss = 0.6683, Val Acc = 0.7923\n",
      "Epoch 19: Train Loss = 0.4738, Val Loss = 0.6678, Val Acc = 0.7918\n",
      "Epoch 20: Train Loss = 0.4713, Val Loss = 0.6700, Val Acc = 0.7894\n",
      "Epoch 21: Train Loss = 0.4692, Val Loss = 0.6723, Val Acc = 0.7918\n",
      "Epoch 22: Train Loss = 0.4679, Val Loss = 0.6767, Val Acc = 0.7915\n",
      "Epoch 23: Train Loss = 0.4662, Val Loss = 0.6813, Val Acc = 0.7895\n",
      "Epoch 24: Train Loss = 0.4646, Val Loss = 0.6628, Val Acc = 0.7941\n",
      "Epoch 25: Train Loss = 0.4634, Val Loss = 0.6593, Val Acc = 0.7949\n",
      "Epoch 26: Train Loss = 0.4613, Val Loss = 0.6851, Val Acc = 0.7903\n",
      "Epoch 27: Train Loss = 0.4601, Val Loss = 0.6881, Val Acc = 0.7914\n",
      "Epoch 28: Train Loss = 0.4591, Val Loss = 0.6858, Val Acc = 0.7899\n",
      "Epoch 29: Train Loss = 0.4578, Val Loss = 0.6810, Val Acc = 0.7915\n",
      "Epoch 30: Train Loss = 0.4565, Val Loss = 0.6749, Val Acc = 0.7921\n"
     ]
    }
   ],
   "source": [
    "mfcc_train_x = np.load(\"lmfcc_train_x_dy.npz\")['X']\n",
    "mfcc_val_x   = np.load(\"lmfcc_val_x_dy.npz\")['X']\n",
    "mfcc_test_x  = np.load(\"lmfcc_test_x_dy.npz\")['X']\n",
    "\n",
    "train_y = np.load(\"train_y_dy.npz\")['y']\n",
    "val_y   = np.load(\"val_y_dy.npz\")['y']\n",
    "test_y  = np.load(\"test_y_dy.npz\")['y']\n",
    "\n",
    "train_x = torch.tensor(mfcc_train_x, dtype=torch.float32)\n",
    "val_x   = torch.tensor(mfcc_val_x,   dtype=torch.float32)\n",
    "test_x  = torch.tensor(mfcc_test_x,  dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "val_y   = torch.tensor(val_y,   dtype=torch.long)\n",
    "test_y  = torch.tensor(test_y,  dtype=torch.long)\n",
    "\n",
    "input_dim = mfcc_train_x.shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = len(torch.unique(train_y))\n",
    "batch_size = 256\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(val_x, val_y),     batch_size=batch_size)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {accuracy:.4f}\")\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Val\": val_loss}, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Val\", accuracy, epoch)\n",
    "\n",
    "torch.save(net.state_dict(), \"model_lmfcc_dy_2layer.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e102d45",
   "metadata": {},
   "source": [
    "# non dynamic lmfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdb3b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.2462, Val Loss = 1.2434, Val Acc = 0.6075\n",
      "Epoch 2: Train Loss = 1.1453, Val Loss = 1.2232, Val Acc = 0.6136\n",
      "Epoch 3: Train Loss = 1.1168, Val Loss = 1.2200, Val Acc = 0.6154\n",
      "Epoch 4: Train Loss = 1.0999, Val Loss = 1.2212, Val Acc = 0.6182\n",
      "Epoch 5: Train Loss = 1.0875, Val Loss = 1.2338, Val Acc = 0.6137\n",
      "Epoch 6: Train Loss = 1.0781, Val Loss = 1.2302, Val Acc = 0.6149\n",
      "Epoch 7: Train Loss = 1.0709, Val Loss = 1.2222, Val Acc = 0.6159\n",
      "Epoch 8: Train Loss = 1.0649, Val Loss = 1.2359, Val Acc = 0.6156\n",
      "Epoch 9: Train Loss = 1.0599, Val Loss = 1.2403, Val Acc = 0.6138\n",
      "Epoch 10: Train Loss = 1.0556, Val Loss = 1.2400, Val Acc = 0.6132\n",
      "Epoch 11: Train Loss = 1.0519, Val Loss = 1.2445, Val Acc = 0.6143\n",
      "Epoch 12: Train Loss = 1.0486, Val Loss = 1.2391, Val Acc = 0.6148\n",
      "Epoch 13: Train Loss = 1.0456, Val Loss = 1.2598, Val Acc = 0.6113\n",
      "Epoch 14: Train Loss = 1.0428, Val Loss = 1.2530, Val Acc = 0.6104\n",
      "Epoch 15: Train Loss = 1.0404, Val Loss = 1.2657, Val Acc = 0.6110\n",
      "Epoch 16: Train Loss = 1.0385, Val Loss = 1.2709, Val Acc = 0.6087\n",
      "Epoch 17: Train Loss = 1.0366, Val Loss = 1.2649, Val Acc = 0.6108\n",
      "Epoch 18: Train Loss = 1.0346, Val Loss = 1.2714, Val Acc = 0.6105\n",
      "Epoch 19: Train Loss = 1.0326, Val Loss = 1.2696, Val Acc = 0.6122\n",
      "Epoch 20: Train Loss = 1.0315, Val Loss = 1.2810, Val Acc = 0.6080\n"
     ]
    }
   ],
   "source": [
    "mfcc_train_x = np.load(\"lmfcc_train_x_nd.npz\")['X']\n",
    "mfcc_val_x   = np.load(\"lmfcc_val_x_nd.npz\")['X']\n",
    "mfcc_test_x  = np.load(\"lmfcc_test_x_nd.npz\")['X']\n",
    "\n",
    "train_y = np.load(\"train_y_nd.npz\")['y']\n",
    "val_y   = np.load(\"val_y_nd.npz\")['y']\n",
    "test_y  = np.load(\"test_y_nd.npz\")['y']\n",
    "\n",
    "train_x = torch.tensor(mfcc_train_x, dtype=torch.float32)\n",
    "val_x   = torch.tensor(mfcc_val_x,   dtype=torch.float32)\n",
    "test_x  = torch.tensor(mfcc_test_x,  dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "val_y   = torch.tensor(val_y,   dtype=torch.long)\n",
    "test_y  = torch.tensor(test_y,  dtype=torch.long)\n",
    "\n",
    "input_dim = mfcc_train_x.shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = len(torch.unique(train_y))\n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(val_x, val_y),     batch_size=batch_size)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {accuracy:.4f}\")\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Val\": val_loss}, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Val\", accuracy, epoch)\n",
    "\n",
    "torch.save(net.state_dict(), \"model_lmfcc_nd_2layer.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929e44e",
   "metadata": {},
   "source": [
    "# N-d mspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c2c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.2692, Val Loss = 1.2167, Val Acc = 0.6185\n",
      "Epoch 2: Train Loss = 1.1310, Val Loss = 1.1718, Val Acc = 0.6320\n",
      "Epoch 3: Train Loss = 1.0908, Val Loss = 1.1653, Val Acc = 0.6326\n",
      "Epoch 4: Train Loss = 1.0659, Val Loss = 1.1884, Val Acc = 0.6318\n",
      "Epoch 5: Train Loss = 1.0489, Val Loss = 1.1822, Val Acc = 0.6309\n",
      "Epoch 6: Train Loss = 1.0367, Val Loss = 1.1873, Val Acc = 0.6313\n",
      "Early stopping triggered at epoch 6\n"
     ]
    }
   ],
   "source": [
    "mspec_train_x = np.load(\"mspec_train_x_nd.npz\")[\"X\"]\n",
    "mspec_val_x   = np.load(\"mspec_val_x_nd.npz\")[\"X\"]\n",
    "mspec_test_x  = np.load(\"mspec_test_x_nd.npz\")[\"X\"]\n",
    "\n",
    "train_y = np.load(\"train_y_nd.npz\")[\"y\"]\n",
    "val_y   = np.load(\"val_y_nd.npz\")[\"y\"]\n",
    "test_y  = np.load(\"test_y_nd.npz\")[\"y\"]\n",
    "\n",
    "train_x = torch.tensor(mspec_train_x, dtype=torch.float32)\n",
    "val_x   = torch.tensor(mspec_val_x, dtype=torch.float32)\n",
    "test_x  = torch.tensor(mspec_test_x, dtype=torch.float32)\n",
    "\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "val_y   = torch.tensor(val_y, dtype=torch.long)\n",
    "test_y  = torch.tensor(test_y, dtype=torch.long)\n",
    "\n",
    "input_dim = mspec_train_x.shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = len(torch.unique(train_y))\n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "patience = 3\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(val_x, val_y),     batch_size=batch_size)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {accuracy:.4f}\")\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Val\": val_loss}, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Val\", accuracy, epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(net.state_dict(), \"model_mspec_nd_2layer.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee332c8",
   "metadata": {},
   "source": [
    "# dynamic mspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a12d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.41 GiB for an array with shape (379634360,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mspec_train_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmspec_train_x_dy.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m mspec_val_x   \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmspec_val_x_dy.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m train_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_y_dy.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda\\main\\Lib\\site-packages\\numpy\\lib\\npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mread_array(\u001b[38;5;28mbytes\u001b[39m,\n\u001b[0;32m    257\u001b[0m                              allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_pickle,\n\u001b[0;32m    258\u001b[0m                              pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpickle_kwargs,\n\u001b[0;32m    259\u001b[0m                              max_header_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_header_size)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[1;32md:\\anaconda\\main\\Lib\\site-packages\\numpy\\lib\\format.py:822\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    809\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfromfile(fp, dtype\u001b[38;5;241m=\u001b[39mdtype, count\u001b[38;5;241m=\u001b[39mcount)\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m--> 822\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    825\u001b[0m         \u001b[38;5;66;03m# If dtype.itemsize == 0 then there's nothing more to read\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         max_read_count \u001b[38;5;241m=\u001b[39m BUFFER_SIZE \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmin\u001b[39m(BUFFER_SIZE, dtype\u001b[38;5;241m.\u001b[39mitemsize)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.41 GiB for an array with shape (379634360,) and data type float32"
     ]
    }
   ],
   "source": [
    "mspec_train_x = np.load(\"mspec_train_x_dy.npz\")[\"X\"]\n",
    "mspec_val_x   = np.load(\"mspec_val_x_dy.npz\")[\"X\"]\n",
    "\n",
    "\n",
    "train_y = np.load(\"train_y_dy.npz\")[\"y\"]\n",
    "val_y   = np.load(\"val_y_dy.npz\")[\"y\"]\n",
    "\n",
    "\n",
    "train_x = torch.tensor(mspec_train_x, dtype=torch.float32)\n",
    "val_x   = torch.tensor(mspec_val_x, dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "val_y   = torch.tensor(val_y, dtype=torch.long)\n",
    "\n",
    "\n",
    "input_dim = mspec_train_x.shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = len(torch.unique(train_y))\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "patience = 5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(val_x, val_y),     batch_size=batch_size)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {accuracy:.4f}\")\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Val\": val_loss}, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Val\", accuracy, epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(net.state_dict(), \"model_mspec_dy_2layer.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d9cb7",
   "metadata": {},
   "source": [
    "# evaluation just for dynamic mspec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
